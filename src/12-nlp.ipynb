{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **12-NLP**"
      ],
      "metadata": {
        "id": "w4C8TjvTU2CI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Instalación y Importaciones**"
      ],
      "metadata": {
        "id": "WS16w4LmVCrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJZbk7siVgxW",
        "outputId": "f361dcfa-cac0-4596-f7b1-db681f237a90"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.12/dist-packages (1.4.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm\n",
        "nltk.download('punkt')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_IFxKourVk6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320cfb0c-c44f-4afe-9ddd-8e3a2917c943"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Abrir Dataset**"
      ],
      "metadata": {
        "id": "QjgmH69266FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url = \"https://breathecode.herokuapp.com/asset/internal-link?id=932&path=url_spam.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "print(df.head())\n",
        "print(df.shape)\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpencrhhVpz1",
        "outputId": "89f10577-19ba-410f-9b11-1c7afe0590d9"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 url  is_spam\n",
            "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
            "1                             https://www.hvper.com/     True\n",
            "2                 https://briefingday.com/m/v4n3i4f3     True\n",
            "3   https://briefingday.com/n/20200618/m#commentform    False\n",
            "4                        https://briefingday.com/fan     True\n",
            "(2999, 2)\n",
            "Index(['url', 'is_spam'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Preprocesamiento de URLS**"
      ],
      "metadata": {
        "id": "PQGZUGbyVDCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "# Función de preprocesamiento para URLs\n",
        "# Función de preprocesamiento de URLs\n",
        "def preprocess_url(url):\n",
        "    url = url.lower()\n",
        "    url = re.sub(r'[^\\w]', ' ', url)  # Separar por signos de puntuación\n",
        "    tokens = url.split()\n",
        "    tokens = [unidecode(token) for token in tokens]\n",
        "\n",
        "    # Stopwords personalizadas para URLs\n",
        "    stop_words = stopwords.words('english')\n",
        "    stop_words.extend(['www', 'com', 'http', 'https', 'html', 'php', 'index'])\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lematización\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Aplicar preprocesamiento\n",
        "df['url_prepro'] = df['url'].progress_apply(preprocess_url)\n",
        "\n",
        "# Revisar resultados\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMyC_MJiVMb1",
        "outputId": "11a4b535-7184-4fb8-aa0d-f61cf92d3792"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2999/2999 [00:09<00:00, 327.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 url  is_spam  \\\n",
            "0  https://briefingday.us8.list-manage.com/unsubs...     True   \n",
            "1                             https://www.hvper.com/     True   \n",
            "2                 https://briefingday.com/m/v4n3i4f3     True   \n",
            "3   https://briefingday.com/n/20200618/m#commentform    False   \n",
            "4                        https://briefingday.com/fan     True   \n",
            "\n",
            "                                url_prepro  \n",
            "0  briefingday us8 list manage unsubscribe  \n",
            "1                                    hvper  \n",
            "2                     briefingday v4n3i4f3  \n",
            "3       briefingday n 20200618 commentform  \n",
            "4                          briefingday fan  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Dividir Dataset**"
      ],
      "metadata": {
        "id": "U7ZxMVmu7IEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = df['url_prepro']\n",
        "y = df['is_spam']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "xhsg3m0WyDBZ"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Vectorización TF-IDF**"
      ],
      "metadata": {
        "id": "W_3Zf_4f8kVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vectorizer = TfidfVectorizer(min_df=0.001)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "print(\"Número de características:\", len(vectorizer.get_feature_names_out()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7mDClrI09xA",
        "outputId": "90ccb987-0697-4ede-80f2-97a70fd2dfd2"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de características: 1186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Entrenamiento de SVM básico**"
      ],
      "metadata": {
        "id": "G0obLkOv8obP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Entrenar SVM con parámetros por defecto\n",
        "clf = SVC()\n",
        "clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predecir\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "\n",
        "# Evaluar resultados\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzjlD-zk1BCJ",
        "outputId": "18e7b889-cd02-4fe9-efb0-669f0ec4722e"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.95      0.98      0.96       455\n",
            "        True       0.94      0.83      0.88       145\n",
            "\n",
            "    accuracy                           0.94       600\n",
            "   macro avg       0.94      0.91      0.92       600\n",
            "weighted avg       0.94      0.94      0.94       600\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Optimización de SVM** mejoro el Modelo. Mayor Accuracy."
      ],
      "metadata": {
        "id": "KXwzuD7Q8xgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_vec, y_train)\n",
        "\n",
        "print(\"Mejores parámetros:\", grid.best_params_)\n",
        "print(\"Mejor score CV:\", grid.best_score_)\n",
        "\n",
        "# Evaluación final\n",
        "y_pred_opt = grid.predict(X_test_vec)\n",
        "print(classification_report(y_test, y_pred_opt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvklS3BS1E6V",
        "outputId": "1106b454-edac-4a3b-a4a9-12ea02529901"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mejores parámetros: {'C': 1, 'gamma': 'scale', 'kernel': 'poly'}\n",
            "Mejor score CV: 0.9649904314544189\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.95      0.99      0.97       455\n",
            "        True       0.95      0.82      0.88       145\n",
            "\n",
            "    accuracy                           0.95       600\n",
            "   macro avg       0.95      0.90      0.92       600\n",
            "weighted avg       0.95      0.95      0.95       600\n",
            "\n"
          ]
        }
      ]
    }
  ]
}